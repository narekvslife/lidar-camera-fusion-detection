{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32498148",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "context has already been set",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a142fdf1ad06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.cuda.FloatTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36mset_start_method\u001b[0;34m(self, method, force)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_start_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actual_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'context has already been set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actual_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: context has already been set"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import RandomRotation\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "from pyquaternion import Quaternion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.prepocess import sample_to_rangeview, pcl_to_rangeview\n",
    "from src.utils import rotation_matrix\n",
    "from src.settings import DATASET_PATH, LABEL_NUMBER, RV_WIDTH, RV_HEIGHT, NUSCENES\n",
    "from src.models.lasernet import LaserNet\n",
    "from src.losses import LaserNetLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f45ae0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1147b71",
   "metadata": {},
   "source": [
    "## Training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb87cf",
   "metadata": {},
   "source": [
    "- ___Classification___ task includes semantic segmentation. We predict class labels for each point (cell) in the Range View. If a cell in RV gets a class C, we extrapolate that all points (which fell into that cell during the transformation to RV) get the same label.\n",
    "- ___Regression___ task includes BB regression and mixture parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960503db",
   "metadata": {},
   "source": [
    "so a single training example consists of: \n",
    "\n",
    "- __$X$__: range_view image | __5 x W x H__\n",
    "\n",
    "\n",
    "- __$Y_{image}$:__ | __C x W x H__, where C - number of classes\n",
    "\n",
    "\n",
    "- __$Y_{bb}$:__ $\\{\\{b_{m,1}, b_{m,2}, b_{m,3}, b_{m,4}\\}, ..., \\}_m^M$ | __M x 4 x 2 x W x H__ | where M is the number of bounding boxes in the image, $b_{m, j} \\in R ^2$ is the absolute coordinate of $m$-th bounding box's $j$-th corner\n",
    "\n",
    "- __$Y_{logstd}$:__ $\\log(\\sigma)$ of the predicted bb coordinates| __scalar__\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea4c82",
   "metadata": {},
   "source": [
    "### DataSets, DataLoaders and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e97accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes\n",
    "from os.path import join\n",
    "\n",
    "class NuscenesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_root, n: tuple=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): root NuScenes directory\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            \n",
    "            n - tuple with left and right index boundaries, in case we don't want to use all data\n",
    "        \"\"\"\n",
    "        assert len(n) == 2\n",
    "        self.data_root = data_root\n",
    "        self.nuscenes = NUSCENES\n",
    "\n",
    "        if n:\n",
    "            self.samples = self.nuscenes.sample[n[0]:n[1]]\n",
    "        else:\n",
    "            self.samples = self.nuscenes.sample\n",
    "            \n",
    "        # point_clouds_features will be of shape (N, M, 5), \n",
    "        # where N - number of samples, M - number of points in according sample's pointcloud\n",
    "        self.point_clouds_features = []\n",
    "        self.point_clouds_labels = []\n",
    "        \n",
    "        self.__set_point_clouds()\n",
    "            \n",
    "    def __set_point_clouds(self):\n",
    "\n",
    "        for sample in self.samples:\n",
    "            \n",
    "            sample_data_token = sample['data']['LIDAR_TOP']\n",
    "\n",
    "            my_sample_lidar_data = self.nuscenes.get('sample_data', sample_data_token)\n",
    "\n",
    "            lidarseg_labels_filename = join(self.data_root,\n",
    "                                            self.nuscenes.get('lidarseg', sample_data_token)['filename'])\n",
    "\n",
    "            # loading directly from files to perceive the ring_index information\n",
    "            points_raw = np.fromfile(self.data_root + my_sample_lidar_data[\"filename\"], dtype=np.float32).reshape((-1, 5))\n",
    "            point_labels = np.fromfile(lidarseg_labels_filename, dtype=np.uint8)\n",
    "            \n",
    "            self.point_clouds_features.append(points_raw)\n",
    "            self.point_clouds_labels.append(point_labels)\n",
    "            \n",
    "        self.point_clouds_features = np.array(self.point_clouds_features)\n",
    "        self.point_clouds_labels = np.array(self.point_clouds_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_front_bb(self, sample: dict):\n",
    "        \"\"\"\n",
    "        Function computes and returns\n",
    "        An array of points points of a bounding box in sensor coordinates.\n",
    "        Each point is a (x, y) coordinate, each BB is 4 points\n",
    "\n",
    "        :param sample: nuscenes sample dictionary\n",
    "        :return: np.array of shape (N, 8, 3)\n",
    "        \"\"\"\n",
    "        my_sample_lidar_data = self.nuscenes.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "        sample_annotation = self.nuscenes.get_boxes(my_sample_lidar_data['token'])\n",
    "\n",
    "        ego_record = self.nuscenes.get('ego_pose', my_sample_lidar_data['ego_pose_token'])\n",
    "        cs_record = self.nuscenes.get('calibrated_sensor', my_sample_lidar_data['calibrated_sensor_token'])\n",
    "\n",
    "        # first step: transform from absolute to ego\n",
    "        ego_translation = -np.array(ego_record['translation'])\n",
    "\n",
    "        # second step: transform from ego to sensor\n",
    "        cs_translation = -np.array(cs_record['translation'])\n",
    "\n",
    "        corners = []\n",
    "        for box in sample_annotation:\n",
    "            box.translate(ego_translation)\n",
    "            box.rotate(Quaternion(ego_record['rotation']).inverse)\n",
    "\n",
    "            box.translate(cs_translation)\n",
    "            box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
    "\n",
    "            # at this point bounding boxes are in sensor coordinate system\n",
    "            # now we want to exclude such BB that do not have their center\n",
    "            # lying in the front 90 degrees\n",
    "\n",
    "            if box.center[1] <= 0:\n",
    "                continue\n",
    "\n",
    "            box.center_azimuth = np.degrees(np.arctan(box.center[0] / box.center[1]))\n",
    "\n",
    "            # Transform front azimuth to be in range from 0 to 180\n",
    "            box.center_azimuth = 90 - box.center_azimuth\n",
    "            if not (45 < box.center_azimuth < 135):\n",
    "                continue\n",
    "\n",
    "            corners.append(box.bottom_corners())\n",
    "        \n",
    "        if len(corners) > 0:\n",
    "            return np.transpose(np.array(corners), (0, 2, 1))\n",
    "        else:\n",
    "            return np.zeros((1, 4, 3))\n",
    "    \n",
    "    def points_in_box(self, coordinates, bounding_box_corners):\n",
    "        \"\"\"\n",
    "            bounding_box_corners: bbc of a single bb\n",
    "            return a mask of whether points that are in the box\n",
    "        \"\"\"\n",
    "        coords_x = coordinates[0]\n",
    "        coords_y = coordinates[1]\n",
    "        \n",
    "        min_bb_x = bounding_box_corners[:, 0].min()\n",
    "        max_bb_x = bounding_box_corners[:, 0].max()\n",
    "        min_bb_y = bounding_box_corners[:, 1].min()\n",
    "        max_bb_y = bounding_box_corners[:, 1].max()\n",
    "\n",
    "        c1 = min_bb_x <= coords_x  # left_top/left_bottom.x <= coordinate.x\n",
    "        c2 = max_bb_x >= coords_x  # right_bottom/right_top.x >= coordinate.x\n",
    "        c3 = min_bb_y <= coords_y  # left/right_bottom.y <= coordinate.y\n",
    "        c4 = max_bb_y >= coords_y  # right_top/left_top.y >= coordinate.y\n",
    "\n",
    "        c = np.logical_and(np.logical_and(c1, c2),\n",
    "                           np.logical_and(c3, c4))\n",
    "        return c\n",
    "    \n",
    "    def get_bb_targets(self, idx, bounding_box_corners):\n",
    "        coordinates = self.point_clouds_features[idx][:2] \n",
    "    \n",
    "        for bb_c in bounding_box_corners:\n",
    "\n",
    "            if self.points_in_box(coordinates, bb_c).any():\n",
    "                return np.array(bb_c[:, :2])\n",
    "            \n",
    "            else:\n",
    "                return np.zeros_like(bb_c[:, :2])\n",
    "    \n",
    "    def __getitem__(self, idx, compute_boxes=True):\n",
    "        \"\"\"\n",
    "        compute_box will be set to False in child classes, so target boxes are computed after rotations\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        sample_idx = self.samples[idx]\n",
    "        pcl_features = self.point_clouds_features[idx]\n",
    "        pcl_labels = self.point_clouds_labels[idx]\n",
    "        \n",
    "        if compute_boxes:\n",
    "            front_bbs = self.get_front_bb(sample_idx)\n",
    "            target_bounding_boxes = self.get_bb_targets(idx, front_bbs)\n",
    "    \n",
    "            return pcl_features, pcl_labels, target_bounding_boxes\n",
    "        \n",
    "        else:\n",
    "            return pcl_features, pcl_labels, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf601d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuscenesRangeViewDataset(NuscenesDataset):\n",
    "\n",
    "    def __init__(self, data_root, n=None):\n",
    "        super().__init__(data_root, n)\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def get_bb_targets(self, range_view_coordinates, bounding_box_corners):\n",
    "        bbc_target = np.zeros((4, 2, RV_WIDTH, RV_HEIGHT))\n",
    "                \n",
    "        for bbc in bounding_box_corners:\n",
    "            point_mask = self.points_in_box(range_view_coordinates, bbc)\n",
    "            bbc_target[:, :, point_mask] = np.expand_dims(bbc, 2)\n",
    "            \n",
    "        return bbc_target.reshape((8, RV_WIDTH, RV_HEIGHT))\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pcl_features, pcl_targets, _ = super().__getitem__(idx, compute_boxes=False)\n",
    "        \n",
    "        rotate_prob = np.random.uniform()\n",
    "        \n",
    "#         if rotate_prob > 0.5:\n",
    "#             rotation_angle_y = np.random.uniform(10, 90)\n",
    "\n",
    "#             rotation = RandomRotation((0, rotation_angle_y))\n",
    "#             print(\"0\", pcl_features[:, :2].shape)\n",
    "#             pcl_features[:, :2] = pcl_features[:, :2] @ rotation_matrix(torch.Tensor([rotation_angle_y])).cpu().numpy()\n",
    "#             print(\"1\", pcl_features[:, :2].shape)\n",
    "            \n",
    "        range_view, targets = pcl_to_rangeview(pcl_features, pcl_targets)\n",
    "        \n",
    "        range_view = range_view.transpose(2, 1, 0)\n",
    "        targets = targets.transpose(2, 1, 0)\n",
    "        \n",
    "        front_bbs = self.get_front_bb(self.samples[idx])[:, :, :2]  # N x 4 x 2 since we only need xy\n",
    "        target_bounding_boxes = self.get_bb_targets(range_view[:2], front_bbs)\n",
    "    \n",
    "        return torch.Tensor(range_view.copy()), torch.Tensor(targets.copy()), torch.Tensor(target_bounding_boxes.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f7f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-79e4882f6273>:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.point_clouds_features = np.array(self.point_clouds_features)\n",
      "<ipython-input-2-79e4882f6273>:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.point_clouds_labels = np.array(self.point_clouds_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 1.22 s, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = NuscenesRangeViewDataset(data_root=DATASET_PATH, n=(0, 4000))\n",
    "val_dataset = NuscenesRangeViewDataset(data_root=DATASET_PATH, n=(4000, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed81ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=64, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751840a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4ea39",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf9341",
   "metadata": {},
   "source": [
    "1. For each point in the image, we use the focal loss $L_{prob}$ to learn the class probabilities $\\{p_c\\}_{c=1}^C$. The classification loss for the entire image is defined as follows \n",
    "$$\n",
    "L_{cls} = {1 \\over P} \\sum_i{L_{prob, i}}\n",
    "$$ \n",
    "where P is the number of points in the image\n",
    "\n",
    "2. For each point on an object, we learn the parameters of the object’s mixture model by first identifying which component best matches the ground truth\n",
    "$$\n",
    "k^* = \\arg \\min_k || \\hat b_k − b^{gt} ||\n",
    "$$\n",
    "where $\\hat b_k$ is the k-th mean component of the mixture model\n",
    "and $b^{gt}$ is the corresponding ground truth bounding box.\n",
    "\n",
    "3. Afterwards, we update the parameters of the $k^{*}$ component\n",
    "$$\n",
    "L_{box} = \\sum_n {1 \\over \\hat \\sigma_{k^*}} | \\hat b_{n, k} − b^{gt}_n | + \\log{\\hat \\sigma_{k^*}}\n",
    "$$\n",
    "\n",
    "\n",
    "4. Next, we update the mixture weights $\\{α_k\\}^K_{k=1}$ again using the multi-class cross entropy loss $L_{mix}$, where the positive label corresponds to the $k^*$ component\n",
    "\n",
    "5. The regression loss for the entire image is defined as follows:\n",
    "$$\n",
    "L_{reg} = {{1 \\over N} \\sum_i{L_{box, i} + \\lambda L_{mix, i} \\over n_i}}\n",
    "$$\n",
    "where $L_{box, i}$ and $L_{mix, i}$ are the losses for the $i$-th point in the image which is on an object, $n_i$ is the total number of points that lie on the same object as $i$, $N$ is the total instances of objects in the image, and $\\lambda$ is the relative weighting of the two losses.\n",
    "\n",
    "6. Final loss is \n",
    "\n",
    "$$\n",
    "L = L_{reg} + L_{cls}\n",
    "$$\n",
    "\n",
    "___!NOTE! In this experiment we do not model a distribution of BB, which is equivalent to having a single mixture component or  K=1. Meaning: we do not have $L_{mix}$, we skip step 2___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82fe4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de15a83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:11<11:56, 11.56s/it]\u001b[A\n",
      "  3%|▎         | 2/63 [00:23<11:52, 11.68s/it]\u001b[A\n",
      "  5%|▍         | 3/63 [00:35<11:57, 11.96s/it]\u001b[A\n",
      "  6%|▋         | 4/63 [00:47<11:51, 12.06s/it]\u001b[A\n",
      "  8%|▊         | 5/63 [00:59<11:29, 11.90s/it]\u001b[A\n",
      " 10%|▉         | 6/63 [01:10<11:09, 11.74s/it]\u001b[A\n",
      " 11%|█         | 7/63 [01:22<10:52, 11.65s/it]\u001b[A\n",
      " 13%|█▎        | 8/63 [01:33<10:33, 11.51s/it]\u001b[A\n",
      " 14%|█▍        | 9/63 [01:45<10:23, 11.54s/it]\u001b[A\n",
      " 16%|█▌        | 10/63 [01:56<10:13, 11.58s/it]\u001b[A\n",
      " 17%|█▋        | 11/63 [02:09<10:14, 11.83s/it]\u001b[A\n",
      " 19%|█▉        | 12/63 [02:21<10:13, 12.03s/it]\u001b[A\n",
      " 21%|██        | 13/63 [02:34<10:08, 12.17s/it]\u001b[A\n",
      " 22%|██▏       | 14/63 [02:46<09:56, 12.17s/it]\u001b[A\n",
      " 24%|██▍       | 15/63 [02:58<09:40, 12.10s/it]\u001b[A\n",
      " 25%|██▌       | 16/63 [03:10<09:31, 12.15s/it]\u001b[A\n",
      " 27%|██▋       | 17/63 [03:22<09:11, 11.99s/it]\u001b[A\n",
      " 29%|██▊       | 18/63 [03:34<09:05, 12.12s/it]\u001b[A\n",
      " 30%|███       | 19/63 [03:46<08:54, 12.15s/it]\u001b[A\n",
      " 32%|███▏      | 20/63 [03:58<08:32, 11.92s/it]\u001b[A\n",
      " 33%|███▎      | 21/63 [04:09<08:10, 11.69s/it]\u001b[A\n",
      " 35%|███▍      | 22/63 [04:20<07:56, 11.63s/it]\u001b[A\n",
      " 37%|███▋      | 23/63 [04:32<07:42, 11.56s/it]\u001b[A\n",
      " 38%|███▊      | 24/63 [04:43<07:26, 11.44s/it]\u001b[A\n",
      " 40%|███▉      | 25/63 [04:54<07:08, 11.28s/it]\u001b[A\n",
      " 41%|████▏     | 26/63 [05:05<06:58, 11.30s/it]\u001b[A\n",
      " 43%|████▎     | 27/63 [05:17<06:52, 11.47s/it]\u001b[A\n",
      " 44%|████▍     | 28/63 [05:29<06:44, 11.56s/it]\u001b[A\n",
      " 46%|████▌     | 29/63 [05:41<06:34, 11.62s/it]\u001b[A\n",
      " 48%|████▊     | 30/63 [05:52<06:22, 11.58s/it]\u001b[A\n",
      " 49%|████▉     | 31/63 [06:04<06:15, 11.72s/it]\u001b[A\n",
      " 51%|█████     | 32/63 [06:17<06:09, 11.93s/it]\u001b[A\n",
      " 52%|█████▏    | 33/63 [06:28<05:50, 11.67s/it]\u001b[A\n",
      " 54%|█████▍    | 34/63 [06:39<05:39, 11.70s/it]\u001b[A\n",
      " 56%|█████▌    | 35/63 [06:51<05:28, 11.75s/it]\u001b[A\n",
      " 57%|█████▋    | 36/63 [07:03<05:21, 11.90s/it]\u001b[A\n",
      " 59%|█████▊    | 37/63 [07:16<05:13, 12.07s/it]\u001b[A\n",
      " 60%|██████    | 38/63 [07:27<04:54, 11.78s/it]\u001b[A\n",
      " 62%|██████▏   | 39/63 [07:39<04:41, 11.72s/it]\u001b[A\n",
      " 63%|██████▎   | 40/63 [07:51<04:33, 11.87s/it]\u001b[A\n",
      " 65%|██████▌   | 41/63 [08:03<04:21, 11.87s/it]\u001b[A\n",
      " 67%|██████▋   | 42/63 [08:14<04:06, 11.76s/it]\u001b[A\n",
      " 68%|██████▊   | 43/63 [08:26<03:55, 11.79s/it]\u001b[A\n",
      " 70%|██████▉   | 44/63 [08:38<03:45, 11.86s/it]\u001b[A\n",
      " 71%|███████▏  | 45/63 [08:50<03:33, 11.85s/it]\u001b[A\n",
      " 73%|███████▎  | 46/63 [09:01<03:18, 11.70s/it]\u001b[A\n",
      " 75%|███████▍  | 47/63 [09:13<03:05, 11.59s/it]\u001b[A\n",
      " 76%|███████▌  | 48/63 [09:23<02:50, 11.35s/it]\u001b[A\n",
      " 78%|███████▊  | 49/63 [09:34<02:36, 11.16s/it]\u001b[A\n",
      " 79%|███████▉  | 50/63 [09:55<02:34, 11.91s/it]\u001b[A\n",
      "  0%|          | 0/2000 [09:55<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [7, 128, 32] at entry 0 and [32, 128, 32] at entry 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c2b2dfcd818d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mval_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_rv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target_bbs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mbatch_pointclass_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_bb_param_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_log_std_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlasernet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_rv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'numpy'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'string_'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [7, 128, 32] at entry 0 and [32, 128, 32] at entry 10"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2000\n",
    "\n",
    "lasernet = torch.nn.DataParallel(LaserNet(), device_ids=[0, 1])\n",
    "loss = LaserNetLoss(focal_loss_reduction='mean')\n",
    "optimizer = torch.optim.Adam(lasernet.parameters(), lr=0.000001)\n",
    "\n",
    "lasernet.zero_grad()\n",
    "loss.zero_grad()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    for batch_rv, batch_labels, batch_target_bbs in tqdm(train_dataloader):\n",
    "        \n",
    "        batch_pointclass_preds, batch_bb_param_preds, batch_log_std_preds = lasernet(x=batch_rv)\n",
    "        \n",
    "        batch_logstd = torch.log(torch.std(batch_bb_param_preds, axis=(1, 2, 3)))        \n",
    "        \n",
    "        L_train = loss(batch_pointclass_preds, batch_bb_param_preds, batch_log_std_preds,\n",
    "                 batch_labels,           batch_target_bbs)\n",
    "        \n",
    "        train_losses.append(L_train.item())\n",
    "        \n",
    "        lasernet.zero_grad()\n",
    "        L_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_rv, batch_labels, batch_target_bbs in val_dataloader:\n",
    "            batch_pointclass_preds, batch_bb_param_preds, batch_log_std_preds = lasernet(x=batch_rv)\n",
    "            \n",
    "            L_val = loss(batch_pointclass_preds, batch_bb_param_preds, batch_log_std_preds,\n",
    "                         batch_labels,           batch_target_bbs)\n",
    "        \n",
    "            val_losses.append(L_val.item())\n",
    "            \n",
    "    if epoch % 7 == 0:\n",
    "        torch.save(lasernet, f'lasernet-d2k-b64-e{epoch}-adam-lre6')\n",
    "        print(epoch, \"train_loss\", L_train.item(), \"val_loss\", L_val.item())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89496af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f336a0c2190>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "621a589b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc73af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         rv_example = batch_rv[7]\n",
    "#         bb_corners_targets_example = batch_target_bbs[7]\n",
    "        \n",
    "#         _, bb_w, bb_h = np.nonzero(bb_corners_targets_example).T\n",
    "#         bb_w, bb_h = np.unique(bb_w.cpu()), np.unique(bb_h.cpu())\n",
    "#         fig = go.Figure(data=[go.Scatter3d(x=rv_example[0].cpu().flatten(),\n",
    "#                                            y=rv_example[1].cpu().flatten(),\n",
    "#                                            z=np.zeros_like(rv_example[2].cpu().flatten()),\n",
    "#                                            mode='markers',\n",
    "#                                            marker=dict(size=2))])\n",
    "\n",
    "#         for wi in np.unique(bb_w):\n",
    "#             for he in np.unique(bb_h):\n",
    "#                 x = bb_corners_targets_example[::2, wi, he]\n",
    "#                 y = bb_corners_targets_example[1::2, wi, he]\n",
    "\n",
    "#                 fig.add_mesh3d(x=x.cpu(), \n",
    "#                                y=y.cpu(),\n",
    "#                                z=np.zeros_like(x.cpu()))\n",
    "\n",
    "#         fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fa425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
