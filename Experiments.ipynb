{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32498148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "Loading nuScenes-lidarseg...\n",
      "32 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "34149 lidarseg,\n",
      "Done loading in 23.316 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 6.6 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import RandomRotation\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "from pyquaternion import Quaternion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.prepocess import sample_to_rangeview, pcl_to_rangeview\n",
    "from src.utils import rotation_matrix\n",
    "from src.settings import DATASET_PATH, LABEL_NUMBER, RV_WIDTH, RV_HEIGHT, NUSCENES\n",
    "from src.models.lasernet import LaserNet\n",
    "from src.losses import LaserNetLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f45ae0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1147b71",
   "metadata": {},
   "source": [
    "## Training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb87cf",
   "metadata": {},
   "source": [
    "- ___Classification___ task includes semantic segmentation. We predict class labels for each point (cell) in the Range View. If a cell in RV gets a class C, we extrapolate that all points (which fell into that cell during the transformation to RV) get the same label.\n",
    "- ___Regression___ task includes BB regression and mixture parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960503db",
   "metadata": {},
   "source": [
    "so a single training example consists of: \n",
    "\n",
    "- __$X$__: range_view image | __5 x W x H__\n",
    "\n",
    "\n",
    "- __$Y_{image}$:__ | __C x W x H__, where C - number of classes\n",
    "\n",
    "\n",
    "- __$Y_{bb}$:__ $\\{\\{b_{m,1}, b_{m,2}, b_{m,3}, b_{m,4}\\}, ..., \\}_m^M$ | __M x 4 x 2 x W x H__ | where M is the number of bounding boxes in the image, $b_{m, j} \\in R ^2$ is the absolute coordinate of $m$-th bounding box's $j$-th corner\n",
    "\n",
    "- __$Y_{logstd}$:__ $\\log(\\sigma)$ of the predicted bb coordinates| __scalar__\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea4c82",
   "metadata": {},
   "source": [
    "### DataSets, DataLoaders and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e97accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes\n",
    "from os.path import join\n",
    "\n",
    "class NuscenesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_root, n=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): root NuScenes directory\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_root = data_root\n",
    "        self.nuscenes = NUSCENES\n",
    "\n",
    "        if n:\n",
    "            self.samples = self.nuscenes.sample[:n]\n",
    "        else:\n",
    "            self.samples = self.nuscenes.sample\n",
    "            \n",
    "        # point_clouds_features will be of shape (N, M, 5), \n",
    "        # where N - number of samples, M - number of points in according sample's pointcloud\n",
    "        self.point_clouds_features = []\n",
    "        self.point_clouds_labels = []\n",
    "        \n",
    "        self.__set_point_clouds()\n",
    "            \n",
    "    def __set_point_clouds(self):\n",
    "\n",
    "        for sample in self.samples:\n",
    "            \n",
    "            sample_data_token = sample['data']['LIDAR_TOP']\n",
    "\n",
    "            my_sample_lidar_data = self.nuscenes.get('sample_data', sample_data_token)\n",
    "\n",
    "            lidarseg_labels_filename = join(self.data_root,\n",
    "                                            self.nuscenes.get('lidarseg', sample_data_token)['filename'])\n",
    "\n",
    "            # loading directly from files to perceive the ring_index information\n",
    "            points_raw = np.fromfile(self.data_root + my_sample_lidar_data[\"filename\"], dtype=np.float32).reshape((-1, 5))\n",
    "            point_labels = np.fromfile(lidarseg_labels_filename, dtype=np.uint8)\n",
    "            \n",
    "            self.point_clouds_features.append(points_raw)\n",
    "            self.point_clouds_labels.append(point_labels)\n",
    "            \n",
    "        self.point_clouds_features = np.array(self.point_clouds_features)\n",
    "        self.point_clouds_labels = np.array(self.point_clouds_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def get_front_bb(self, sample: dict):\n",
    "        \"\"\"\n",
    "        Function computes and returns\n",
    "        An array of points points of a bounding box in sensor coordinates.\n",
    "        Each point is a (x, y) coordinate, each BB is 4 points\n",
    "\n",
    "        :param sample: nuscenes sample dictionary\n",
    "        :return: np.array of shape (N, 8, 3)\n",
    "        \"\"\"\n",
    "        my_sample_lidar_data = self.nuscenes.get('sample_data', sample['data']['LIDAR_TOP'])\n",
    "        sample_annotation = self.nuscenes.get_boxes(my_sample_lidar_data['token'])\n",
    "\n",
    "        ego_record = self.nuscenes.get('ego_pose', my_sample_lidar_data['ego_pose_token'])\n",
    "        cs_record = self.nuscenes.get('calibrated_sensor', my_sample_lidar_data['calibrated_sensor_token'])\n",
    "\n",
    "        # first step: transform from absolute to ego\n",
    "        ego_translation = -np.array(ego_record['translation'])\n",
    "\n",
    "        # second step: transform from ego to sensor\n",
    "        cs_translation = -np.array(cs_record['translation'])\n",
    "\n",
    "        corners = []\n",
    "        for box in sample_annotation:\n",
    "            box.translate(ego_translation)\n",
    "            box.rotate(Quaternion(ego_record['rotation']).inverse)\n",
    "\n",
    "            box.translate(cs_translation)\n",
    "            box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
    "\n",
    "            # at this point bounding boxes are in sensor coordinate system\n",
    "            # now we want to exclude such BB that do not have their center\n",
    "            # lying in the front 90 degrees\n",
    "\n",
    "            if box.center[1] <= 0:\n",
    "                continue\n",
    "\n",
    "            box.center_azimuth = np.degrees(np.arctan(box.center[0] / box.center[1]))\n",
    "\n",
    "            # Transform front azimuth to be in range from 0 to 180\n",
    "            box.center_azimuth = 90 - box.center_azimuth\n",
    "            if not (45 < box.center_azimuth < 135):\n",
    "                continue\n",
    "\n",
    "            corners.append(box.bottom_corners())\n",
    "        \n",
    "        if len(corners) > 0:\n",
    "            return np.transpose(np.array(corners), (0, 2, 1))\n",
    "        else:\n",
    "            return np.zeros((1, 4, 3))\n",
    "    \n",
    "    def points_in_box(self, coordinates, bounding_box_corners):\n",
    "        \"\"\"\n",
    "            bounding_box_corners: bbc of a single bb\n",
    "            return a mask of whether points that are in the box\n",
    "        \"\"\"\n",
    "        coords_x = coordinates[0]\n",
    "        coords_y = coordinates[1]\n",
    "        \n",
    "        min_bb_x = bounding_box_corners[:, 0].min()\n",
    "        max_bb_x = bounding_box_corners[:, 0].max()\n",
    "        min_bb_y = bounding_box_corners[:, 1].min()\n",
    "        max_bb_y = bounding_box_corners[:, 1].max()\n",
    "\n",
    "        c1 = min_bb_x <= coords_x  # left_top/left_bottom.x <= coordinate.x\n",
    "        c2 = max_bb_x >= coords_x  # right_bottom/right_top.x >= coordinate.x\n",
    "        c3 = min_bb_y <= coords_y  # left/right_bottom.y <= coordinate.y\n",
    "        c4 = max_bb_y >= coords_y  # right_top/left_top.y >= coordinate.y\n",
    "\n",
    "        c = np.logical_and(np.logical_and(c1, c2),\n",
    "                           np.logical_and(c3, c4))\n",
    "        return c\n",
    "    \n",
    "    def get_bb_targets(self, idx, bounding_box_corners):\n",
    "        coordinates = self.point_clouds_features[idx][:2] \n",
    "    \n",
    "        for bb_c in bounding_box_corners:\n",
    "\n",
    "            if self.points_in_box(coordinates, bb_c).any():\n",
    "                return np.array(bb_c[:, :2])\n",
    "            \n",
    "            else:\n",
    "                return np.zeros_like(bb_c[:, :2])\n",
    "    \n",
    "    def __getitem__(self, idx, compute_boxes=True):\n",
    "        \"\"\"\n",
    "        compute_box will be set to False in child classes, so target boxes are computed after rotations\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        sample_idx = self.samples[idx]\n",
    "        pcl_features = self.point_clouds_features[idx]\n",
    "        pcl_labels = self.point_clouds_labels[idx]\n",
    "        \n",
    "        if compute_boxes:\n",
    "            front_bbs = self.get_front_bb(sample_idx)\n",
    "            target_bounding_boxes = self.get_bb_targets(idx, front_bbs)\n",
    "    \n",
    "            return pcl_features, pcl_labels, target_bounding_boxes\n",
    "        \n",
    "        else:\n",
    "            return pcl_features, pcl_labels, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf601d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NuscenesRangeViewDataset(NuscenesDataset):\n",
    "\n",
    "    def __init__(self, data_root, n=None):\n",
    "        super().__init__(data_root, n)\n",
    "        \n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def get_bb_targets(self, range_view_coordinates, bounding_box_corners):\n",
    "        bbc_target = np.zeros((4, 2, RV_WIDTH, RV_HEIGHT))\n",
    "                \n",
    "        for bbc in bounding_box_corners:\n",
    "            point_mask = self.points_in_box(range_view_coordinates, bbc)\n",
    "            bbc_target[:, :, point_mask] = np.expand_dims(bbc, 2)\n",
    "            \n",
    "        return bbc_target.reshape((8, RV_WIDTH, RV_HEIGHT))\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        pcl_features, pcl_targets, _ = super().__getitem__(idx, compute_boxes=False)\n",
    "        \n",
    "        rotate_prob = np.random.uniform()\n",
    "        \n",
    "        if rotate_prob > 0.5:\n",
    "            rotation_angle_y = np.random.uniform(10, 90)\n",
    "\n",
    "#             rotation = RandomRotation((0, rotation_angle_y))\n",
    "#             print(\"0\", pcl_features[:, :2].shape)\n",
    "#             pcl_features[:, :2] = pcl_features[:, :2] @ rotation_matrix(torch.Tensor([rotation_angle_y])).cpu().numpy()\n",
    "#             print(\"1\", pcl_features[:, :2].shape)\n",
    "            \n",
    "        range_view, targets = pcl_to_rangeview(pcl_features, pcl_targets)\n",
    "        \n",
    "        range_view = range_view.transpose(2, 1, 0)\n",
    "        targets = targets.transpose(2, 1, 0)\n",
    "        \n",
    "        front_bbs = self.get_front_bb(self.samples[idx])[:, :, :2]  # N x 4 x 2 since we only need xy\n",
    "        target_bounding_boxes = self.get_bb_targets(range_view[:2], front_bbs)\n",
    "    \n",
    "        return torch.Tensor(range_view.copy()), torch.Tensor(targets.copy()), torch.Tensor(target_bounding_boxes.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f7f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 2.55 ms, total: 2.55 ms\n",
      "Wall time: 2.12 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-361179ed3d67>:46: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.point_clouds_features = np.array(self.point_clouds_features)\n",
      "<ipython-input-2-361179ed3d67>:47: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.point_clouds_labels = np.array(self.point_clouds_labels)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = NuscenesRangeViewDataset(data_root=DATASET_PATH, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed81ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset, batch_size=64, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2751840a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b4ea39",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbf9341",
   "metadata": {},
   "source": [
    "1. For each point in the image, we use the focal loss $L_{prob}$ to learn the class probabilities $\\{p_c\\}_{c=1}^C$. The classification loss for the entire image is defined as follows \n",
    "$$\n",
    "L_{cls} = {1 \\over P} \\sum_i{L_{prob, i}}\n",
    "$$ \n",
    "where P is the number of points in the image\n",
    "\n",
    "2. For each point on an object, we learn the parameters of the object’s mixture model by first identifying which component best matches the ground truth\n",
    "$$\n",
    "k^* = \\arg \\min_k || \\hat b_k − b^{gt} ||\n",
    "$$\n",
    "where $\\hat b_k$ is the k-th mean component of the mixture model\n",
    "and $b^{gt}$ is the corresponding ground truth bounding box.\n",
    "\n",
    "3. Afterwards, we update the parameters of the $k^{*}$ component\n",
    "$$\n",
    "L_{box} = \\sum_n {1 \\over \\hat \\sigma_{k^*}} | \\hat b_{n, k} − b^{gt}_n | + \\log{\\hat \\sigma_{k^*}}\n",
    "$$\n",
    "\n",
    "\n",
    "4. Next, we update the mixture weights $\\{α_k\\}^K_{k=1}$ again using the multi-class cross entropy loss $L_{mix}$, where the positive label corresponds to the $k^*$ component\n",
    "\n",
    "5. The regression loss for the entire image is defined as follows:\n",
    "$$\n",
    "L_{reg} = {{1 \\over N} \\sum_i{L_{box, i} + \\lambda L_{mix, i} \\over n_i}}\n",
    "$$\n",
    "where $L_{box, i}$ and $L_{mix, i}$ are the losses for the $i$-th point in the image which is on an object, $n_i$ is the total number of points that lie on the same object as $i$, $N$ is the total instances of objects in the image, and $\\lambda$ is the relative weighting of the two losses.\n",
    "\n",
    "6. Final loss is \n",
    "\n",
    "$$\n",
    "L = L_{reg} + L_{cls}\n",
    "$$\n",
    "\n",
    "___!NOTE! In this experiment we do not model a distribution of BB, which is equivalent to having a single mixture component or  K=1. Meaning: we do not have $L_{mix}$, we skip step 2___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf82fe4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de15a83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 868712.4375\n",
      "1 820207.9375\n",
      "2 755726.6875\n",
      "3 850533.625\n",
      "4 753860.125\n",
      "5 648676.4375\n",
      "6 506246.90625\n",
      "7 627736.5\n",
      "8 385751.71875\n",
      "9 461532.09375\n",
      "10 550842.3125\n",
      "11 461068.90625\n",
      "12 422648.09375\n",
      "13 422163.3125\n",
      "14 285910.625\n",
      "15 330593.3125\n",
      "16 301820.15625\n",
      "17 230810.65625\n",
      "18 252703.40625\n",
      "19 269587.15625\n",
      "20 193974.65625\n",
      "21 139593.046875\n",
      "22 222016.796875\n",
      "23 195103.3125\n",
      "24 128326.1875\n",
      "25 178735.796875\n",
      "26 112518.6015625\n",
      "27 148121.734375\n",
      "28 102574.578125\n",
      "29 130789.7421875\n",
      "30 135110.875\n",
      "31 75324.5\n",
      "32 81201.828125\n",
      "33 74132.8984375\n",
      "34 79480.9765625\n",
      "35 78041.8984375\n",
      "36 50548.71484375\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2000\n",
    "\n",
    "lasernet = torch.nn.DataParallel(LaserNet(), device_ids=[0, 1])\n",
    "loss = LaserNetLoss(focal_loss_reduction='none')\n",
    "optimizer = torch.optim.Adam(lasernet.parameters(), lr=0.000001)\n",
    "\n",
    "lasernet.zero_grad()\n",
    "loss.zero_grad()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_rv, batch_labels, batch_target_bbs in train_dataloader:\n",
    "        \n",
    "        batch_pointclass_preds, batch_bb_param_preds, batch_log_std_preds = lasernet(x=batch_rv)\n",
    "        batch_logstd = torch.log(torch.std(batch_bb_param_preds, axis=(1, 2, 3)))        \n",
    "        \n",
    "        L = loss(batch_pointclass_preds, batch_bb_param_preds, batch_log_std_preds,\n",
    "                 batch_labels,           batch_target_bbs,     batch_logstd)\n",
    "        \n",
    "        losses.append(L.item())\n",
    "\n",
    "        lasernet.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, L.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc73af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         rv_example = batch_rv[7]\n",
    "#         bb_corners_targets_example = batch_target_bbs[7]\n",
    "        \n",
    "#         _, bb_w, bb_h = np.nonzero(bb_corners_targets_example).T\n",
    "#         bb_w, bb_h = np.unique(bb_w.cpu()), np.unique(bb_h.cpu())\n",
    "#         fig = go.Figure(data=[go.Scatter3d(x=rv_example[0].cpu().flatten(),\n",
    "#                                            y=rv_example[1].cpu().flatten(),\n",
    "#                                            z=np.zeros_like(rv_example[2].cpu().flatten()),\n",
    "#                                            mode='markers',\n",
    "#                                            marker=dict(size=2))])\n",
    "\n",
    "#         for wi in np.unique(bb_w):\n",
    "#             for he in np.unique(bb_h):\n",
    "#                 x = bb_corners_targets_example[::2, wi, he]\n",
    "#                 y = bb_corners_targets_example[1::2, wi, he]\n",
    "\n",
    "#                 fig.add_mesh3d(x=x.cpu(), \n",
    "#                                y=y.cpu(),\n",
    "#                                z=np.zeros_like(x.cpu()))\n",
    "\n",
    "#         fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3fa425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
